{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# Fine-tuning - Prediction de gravite des PASSAGERS par vehicule\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Ce notebook fait suite au notebook **4-a-passengers-ml-training** qui a etabli :\n",
    "- Les modeles specialises sont meilleurs pour : **vehicule_leger** (+11.4% F1), **velo_edp** (+3.1% F1), **pieton** (+6.9% F1)\n",
    "- Le modele global est meilleur pour : voiture et poids_lourd (manque de patterns specifiques)\n",
    "\n",
    "## Decision d'architecture\n",
    "\n",
    "| Categorie | Modele utilise | Justification |\n",
    "|-----------|----------------|---------------|\n",
    "| **vehicule_leger** | Specialise (fine-tune) | F1 0.640 vs 0.526 global |\n",
    "| **velo_edp** | Specialise (fine-tune) | F1 0.557 vs 0.526 global |\n",
    "| **pieton** | Specialise (fine-tune) | F1 0.596 vs 0.526 global |\n",
    "| voiture | Global | F1 global meilleur |\n",
    "| poids_lourd | Global | F1 global meilleur |\n",
    "\n",
    "## Objectif de ce notebook\n",
    "\n",
    "Optimiser les hyperparametres des 3 modeles specialises via Hyperopt pour ameliorer leurs performances.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "```\n",
    "1. Configuration et imports\n",
    "2. Chargement des 3 datasets (vehicule_leger, velo_edp, pieton)\n",
    "3. Fine-tuning Hyperopt pour chaque categorie\n",
    "   - RandomForest\n",
    "   - XGBoost\n",
    "   - LightGBM\n",
    "   - CatBoost\n",
    "4. Comparaison et selection du meilleur modele par categorie\n",
    "5. Sauvegarde des modeles optimises\n",
    "```\n",
    "\n",
    "## Criteres de selection\n",
    "\n",
    "| Critere | Metrique | Justification |\n",
    "|---------|----------|---------------|\n",
    "| **Principal** | F1-Score | Equilibre precision/rappel |\n",
    "| **Secondaire** | AUC | Departage en cas d'egalite |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-config-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Configuration et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Suivi de formation\\simplon-ai-developer-training\\W31-ML-DEPLOYMENT\\ML\\.venv\\Lib\\site-packages\\hyperopt\\atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: 100 estimators, 1 workers, 50 evals Hyperopt\n",
      "Categories a fine-tuner: ['vehicule_leger', 'velo_edp', 'pieton']\n",
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "from ml_config import nb_workers, base_estimators, max_evals\n",
    "from functions import (\n",
    "    display_metrics,\n",
    "    optimize_boosting_model,\n",
    "    plot_optimization_history,\n",
    "    select_best_model,\n",
    "    save_best_model,\n",
    ")\n",
    "\n",
    "# Visualisation\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Categories a fine-tuner (celles ou le modele specialise est meilleur)\n",
    "CATEGORIES_TO_FINETUNE = [\"vehicule_leger\", \"velo_edp\", \"pieton\"]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated\")\n",
    "print(f\"Configuration: {base_estimators} estimators, {nb_workers} workers, {max_evals} evals Hyperopt\")\n",
    "print(f\"Categories a fine-tuner: {CATEGORIES_TO_FINETUNE}\")\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-data-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Chargement des donnees et fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHARGEMENT DES DATASETS ===\n",
      "dataset_passager_vehicule_leger: charge depuis Parquet (101,403 lignes, 18 colonnes)\n",
      "dataset_passager_velo_edp: charge depuis Parquet (37,073 lignes, 16 colonnes)\n",
      "dataset_passager_pieton: charge depuis Parquet (46,080 lignes, 15 colonnes)\n",
      "\n",
      "=== DISTRIBUTION DE LA GRAVITE ===\n",
      "Categorie                Lignes   Taux grave   Colonnes\n",
      "-------------------------------------------------------\n",
      "vehicule_leger          101,403        33.9%         18\n",
      "velo_edp                 37,073        24.6%         16\n",
      "pieton                   46,080        32.7%         15\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(name, base_path=\"data/output\"):\n",
    "    \"\"\"\n",
    "    Charge un dataset depuis Parquet (prioritaire) ou CSV (fallback).\n",
    "    \"\"\"\n",
    "    parquet_path = f\"{base_path}/{name}.parquet\"\n",
    "    csv_path = f\"{base_path}/{name}.csv\"\n",
    "\n",
    "    if os.path.exists(parquet_path):\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        print(f\"{name}: charge depuis Parquet ({df.shape[0]:,} lignes, {df.shape[1]} colonnes)\")\n",
    "    elif os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path, sep=\";\", decimal=\",\", encoding=\"utf-8-sig\")\n",
    "        print(f\"{name}: charge depuis CSV ({df.shape[0]:,} lignes, {df.shape[1]} colonnes)\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset {name} non trouve\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Chargement des datasets a fine-tuner\n",
    "print(\"=== CHARGEMENT DES DATASETS ===\")\n",
    "datasets = {}\n",
    "for cat in CATEGORIES_TO_FINETUNE:\n",
    "    datasets[cat] = load_dataset(f\"dataset_passager_{cat}\")\n",
    "\n",
    "# Affichage des statistiques\n",
    "print(\"\\n=== DISTRIBUTION DE LA GRAVITE ===\")\n",
    "print(f\"{'Categorie':<20} {'Lignes':>10} {'Taux grave':>12} {'Colonnes':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for cat, df in datasets.items():\n",
    "    taux = df[\"grav_binary\"].mean() * 100\n",
    "    print(f\"{cat:<20} {len(df):>10,} {taux:>11.1f}% {df.shape[1]:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonctions definies\n"
     ]
    }
   ],
   "source": [
    "# === FONCTIONS DE PREPARATION DES DONNEES ===\n",
    "\n",
    "\n",
    "def prepare_data(df, target_col=\"grav_binary\"):\n",
    "    \"\"\"\n",
    "    Prepare les features (X) et la target (y) pour l'entrainement.\n",
    "\n",
    "    - Exclut les colonnes de gravite (evite le data leakage)\n",
    "    - Encode les variables categorielles en entiers\n",
    "    \"\"\"\n",
    "    cols_to_drop = [\"grav_ordered\", \"grav_binary\"]\n",
    "    cols_to_drop = [c for c in cols_to_drop if c in df.columns]\n",
    "\n",
    "    X = df.drop(columns=cols_to_drop)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Encodage des variables categorielles\n",
    "    for col in X.select_dtypes(include=[\"object\", \"string\"]).columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_pipeline(model):\n",
    "    \"\"\"\n",
    "    Cree un pipeline sklearn : Imputation -> Scaling -> Modele.\n",
    "    \"\"\"\n",
    "    return Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Entraine un modele et calcule les metriques d'evaluation.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_proba = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"f1\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "    }\n",
    "\n",
    "    if len(np.unique(y_test)) == 2 and y_proba is not None:\n",
    "        results[\"auc\"] = roc_auc_score(y_test, y_proba[:, 1])\n",
    "\n",
    "    return results, y_pred, y_proba\n",
    "\n",
    "\n",
    "def evaluate_catboost(catboost_model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evalue CatBoost SANS Pipeline sklearn (incompatible sklearn 1.6+).\n",
    "    \"\"\"\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_imp = imputer.fit_transform(X_train)\n",
    "    X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "    X_test_scaled = scaler.transform(X_test_imp)\n",
    "\n",
    "    catboost_model.fit(X_train_scaled, y_train)\n",
    "    y_pred = catboost_model.predict(X_test_scaled)\n",
    "\n",
    "    y_proba = None\n",
    "    if hasattr(catboost_model, \"predict_proba\"):\n",
    "        y_proba = catboost_model.predict_proba(X_test_scaled)\n",
    "\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"f1\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "    }\n",
    "\n",
    "    if len(np.unique(y_test)) == 2 and y_proba is not None:\n",
    "        results[\"auc\"] = roc_auc_score(y_test, y_proba[:, 1])\n",
    "\n",
    "    return results, y_pred, y_proba\n",
    "\n",
    "\n",
    "print(\"Fonctions definies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-vehicule-leger-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Fine-tuning VEHICULE_LEGER (2RM, quads)\n",
    "\n",
    "### Caracteristiques de cette categorie\n",
    "\n",
    "| Caracteristique | Valeur |\n",
    "|-----------------|--------|\n",
    "| Taille dataset | ~101k lignes |\n",
    "| Taux de gravite | ~34% |\n",
    "| F1 baseline (4a) | 0.640 |\n",
    "| Objectif | Depasser 0.640 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-vl-prepare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARATION VEHICULE_LEGER ===\n",
      "Features: ['age', 'sexe', 'catu', 'dep', 'region', 'agg', 'vma', 'atm', 'est_nuit', 'est_heure_pointe', 'jour_semaine', 'est_weekend', 'a_equipement_adapte', 'catv', 'sexe_conducteur', 'age_conducteur']\n",
      "\n",
      "Train: 81,122 | Test: 20,281\n",
      "Taux grave train: 33.9%\n",
      "Taux grave test: 33.9%\n",
      "\n",
      "Hyperopt sur 30,000 echantillons\n"
     ]
    }
   ],
   "source": [
    "# === PREPARATION VEHICULE_LEGER ===\n",
    "cat = \"vehicule_leger\"\n",
    "print(f\"=== PREPARATION {cat.upper()} ===\")\n",
    "\n",
    "df_vl = datasets[cat]\n",
    "X_vl, y_vl = prepare_data(df_vl)\n",
    "\n",
    "# Split train/test stratifie\n",
    "X_train_vl, X_test_vl, y_train_vl, y_test_vl = train_test_split(\n",
    "    X_vl, y_vl, test_size=0.2, random_state=RANDOM_STATE, stratify=y_vl\n",
    ")\n",
    "\n",
    "print(f\"Features: {X_vl.columns.tolist()}\")\n",
    "print(f\"\\nTrain: {len(X_train_vl):,} | Test: {len(X_test_vl):,}\")\n",
    "print(f\"Taux grave train: {y_train_vl.mean() * 100:.1f}%\")\n",
    "print(f\"Taux grave test: {y_test_vl.mean() * 100:.1f}%\")\n",
    "\n",
    "# Sous-echantillonnage pour Hyperopt (accelerer)\n",
    "sample_size_vl = min(30000, len(X_train_vl))\n",
    "X_sample_vl = X_train_vl.sample(n=sample_size_vl, random_state=RANDOM_STATE)\n",
    "y_sample_vl = y_train_vl.loc[X_sample_vl.index]\n",
    "print(f\"\\nHyperopt sur {sample_size_vl:,} echantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-vl-hyperopt-rf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HYPEROPT RandomForest - VEHICULE_LEGER\n",
      "============================================================\n",
      "Optimisation Hyperopt pour RANDOMFOREST...\n",
      "  - max_evals: 50\n",
      "  - cv: 3 folds\n",
      "  - scoring: f1\n",
      "\n",
      "100%|██████████| 50/50 [28:14<00:00, 33.89s/trial, best loss: -0.6243240140888174]\n",
      "\n",
      "Meilleurs parametres RANDOMFOREST:\n",
      "  - max_depth: 1\n",
      "  - max_features: 2\n",
      "  - min_samples_leaf: 9\n",
      "  - min_samples_split: 6\n",
      "  - n_estimators: 350\n",
      "\n",
      "Meilleur f1 (CV): 0.6243 (+/- 0.0066)\n",
      "\n",
      "Validation sur test set complet...\n",
      "F1 sur test set: 0.671\n"
     ]
    }
   ],
   "source": [
    "# === HYPEROPT RANDOMFOREST - VEHICULE_LEGER ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT RandomForest - VEHICULE_LEGER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_rf_vl, best_f1_rf_vl, trials_rf_vl = optimize_boosting_model(\n",
    "    X_sample_vl,\n",
    "    y_sample_vl,\n",
    "    model_type=\"randomforest\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "# Validation sur test set complet\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "model_rf_vl = create_pipeline(\n",
    "    RandomForestClassifier(**best_params_rf_vl, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\")\n",
    ")\n",
    "model_rf_vl.fit(X_train_vl, y_train_vl)\n",
    "y_pred_rf_vl = model_rf_vl.predict(X_test_vl)\n",
    "f1_rf_vl_test = f1_score(y_test_vl, y_pred_rf_vl, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_rf_vl_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vl-hyperopt-xgb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HYPEROPT XGBoost - VEHICULE_LEGER\n",
      "============================================================\n",
      "Optimisation Hyperopt pour XGBOOST...\n",
      "  - max_evals: 50\n",
      "  - cv: 3 folds\n",
      "  - scoring: f1\n",
      "\n",
      "100%|██████████| 50/50 [02:10<00:00,  2.61s/trial, best loss: -0.632011917223157] \n",
      "\n",
      "Meilleurs parametres XGBOOST:\n",
      "  - colsample_bytree: 0.9951652709641787\n",
      "  - learning_rate: 0.02386356986404833\n",
      "  - max_depth: 5\n",
      "  - min_child_weight: 2\n",
      "  - n_estimators: 350\n",
      "  - subsample: 0.9124351334655565\n",
      "\n",
      "Meilleur f1 (CV): 0.6320 (+/- 0.0058)\n",
      "\n",
      "Validation sur test set complet...\n",
      "F1 sur test set: 0.717\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === HYPEROPT XGBOOST - VEHICULE_LEGER ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT XGBoost - VEHICULE_LEGER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_xgb_vl, best_f1_xgb_vl, trials_xgb_vl = optimize_boosting_model(\n",
    "    X_sample_vl,\n",
    "    y_sample_vl,\n",
    "    model_type=\"xgboost\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "# Validation sur test set complet\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_vl_imp = pd.DataFrame(imputer.fit_transform(X_train_vl), columns=X_train_vl.columns)\n",
    "X_test_vl_imp = pd.DataFrame(imputer.transform(X_test_vl), columns=X_test_vl.columns)\n",
    "\n",
    "n_neg = (y_train_vl == 0).sum()\n",
    "n_pos = (y_train_vl == 1).sum()\n",
    "scale_pos_weight_vl = n_neg / n_pos\n",
    "\n",
    "model_xgb_vl = XGBClassifier(\n",
    "    **best_params_xgb_vl, random_state=RANDOM_STATE, scale_pos_weight=scale_pos_weight_vl, verbosity=0\n",
    ")\n",
    "model_xgb_vl.fit(X_train_vl_imp, y_train_vl)\n",
    "y_pred_xgb_vl = model_xgb_vl.predict(X_test_vl_imp)\n",
    "f1_xgb_vl_test = f1_score(y_test_vl, y_pred_xgb_vl, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_xgb_vl_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vl-hyperopt-lgbm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HYPEROPT LightGBM - VEHICULE_LEGER\n",
      "============================================================\n",
      "Optimisation Hyperopt pour LIGHTGBM...\n",
      "  - max_evals: 50\n",
      "  - cv: 3 folds\n",
      "  - scoring: f1\n",
      "\n",
      " 24%|██▍       | 12/50 [00:29<01:16,  2.00s/trial, best loss: -0.5575626806948394]"
     ]
    }
   ],
   "source": [
    "# === HYPEROPT LIGHTGBM - VEHICULE_LEGER ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT LightGBM - VEHICULE_LEGER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_lgbm_vl, best_f1_lgbm_vl, trials_lgbm_vl = optimize_boosting_model(\n",
    "    X_sample_vl,\n",
    "    y_sample_vl,\n",
    "    model_type=\"lightgbm\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "# Validation sur test set complet\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "model_lgbm_vl = create_pipeline(\n",
    "    LGBMClassifier(**best_params_lgbm_vl, random_state=RANDOM_STATE, class_weight=\"balanced\", verbose=-1)\n",
    ")\n",
    "model_lgbm_vl.fit(X_train_vl, y_train_vl)\n",
    "y_pred_lgbm_vl = model_lgbm_vl.predict(X_test_vl)\n",
    "f1_lgbm_vl_test = f1_score(y_test_vl, y_pred_lgbm_vl, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_lgbm_vl_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vl-hyperopt-cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPEROPT CATBOOST - VEHICULE_LEGER ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT CatBoost - VEHICULE_LEGER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_cb_vl, best_f1_cb_vl, trials_cb_vl = optimize_boosting_model(\n",
    "    X_sample_vl,\n",
    "    y_sample_vl,\n",
    "    model_type=\"catboost\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "# Validation sur test set complet\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_vl_imp = pd.DataFrame(imputer.fit_transform(X_train_vl), columns=X_train_vl.columns)\n",
    "X_test_vl_imp = pd.DataFrame(imputer.transform(X_test_vl), columns=X_test_vl.columns)\n",
    "\n",
    "model_cb_vl = CatBoostClassifier(\n",
    "    **best_params_cb_vl,\n",
    "    random_state=RANDOM_STATE,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "model_cb_vl.fit(X_train_vl_imp, y_train_vl)\n",
    "y_pred_cb_vl = model_cb_vl.predict(X_test_vl_imp)\n",
    "f1_cb_vl_test = f1_score(y_test_vl, y_pred_cb_vl, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_cb_vl_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vl-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPARAISON VEHICULE_LEGER ===\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARAISON DES MODELES - VEHICULE_LEGER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models_eval_vl = []\n",
    "\n",
    "# RF optimise\n",
    "model = create_pipeline(\n",
    "    RandomForestClassifier(**best_params_rf_vl, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\")\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_model(model, X_train_vl, X_test_vl, y_train_vl, y_test_vl, \"RF_HyperOpt\")\n",
    "models_eval_vl.append({\"name\": \"RF_HyperOpt\", \"y_true\": y_test_vl, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# XGBoost optimise\n",
    "model = create_pipeline(\n",
    "    XGBClassifier(\n",
    "        **best_params_xgb_vl, random_state=RANDOM_STATE, n_jobs=nb_workers, scale_pos_weight=scale_pos_weight_vl\n",
    "    )\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_model(model, X_train_vl, X_test_vl, y_train_vl, y_test_vl, \"XGBoost\")\n",
    "models_eval_vl.append({\"name\": \"XGBoost\", \"y_true\": y_test_vl, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# LightGBM optimise\n",
    "model = create_pipeline(\n",
    "    LGBMClassifier(\n",
    "        **best_params_lgbm_vl, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\", verbose=-1\n",
    "    )\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_model(model, X_train_vl, X_test_vl, y_train_vl, y_test_vl, \"LightGBM\")\n",
    "models_eval_vl.append({\"name\": \"LightGBM\", \"y_true\": y_test_vl, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# CatBoost optimise\n",
    "model_cb = CatBoostClassifier(\n",
    "    **best_params_cb_vl,\n",
    "    random_state=RANDOM_STATE,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_catboost(model_cb, X_train_vl, X_test_vl, y_train_vl, y_test_vl, \"CatBoost\")\n",
    "models_eval_vl.append({\"name\": \"CatBoost\", \"y_true\": y_test_vl, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# Affichage\n",
    "display_metrics(models_results=models_eval_vl, class_labels=[\"Non grave\", \"Grave\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vl-select",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SELECTION MEILLEUR MODELE VEHICULE_LEGER ===\n",
    "best_model_vl = select_best_model(models_eval_vl, metric=\"f1\")\n",
    "print(f\"\\nMeilleur modele VEHICULE_LEGER: {best_model_vl['name']}\")\n",
    "print(f\"F1-Score: {best_model_vl['score']:.4f}\")\n",
    "print(f\"Amelioration vs baseline (0.640): {(best_model_vl['score'] - 0.640) * 100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-velo-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Fine-tuning VELO_EDP (velos, trottinettes)\n",
    "\n",
    "### Caracteristiques de cette categorie\n",
    "\n",
    "| Caracteristique | Valeur |\n",
    "|-----------------|--------|\n",
    "| Taille dataset | ~37k lignes |\n",
    "| Taux de gravite | ~25% |\n",
    "| F1 baseline (4a) | 0.557 |\n",
    "| Objectif | Depasser 0.557 |\n",
    "\n",
    "**Note**: Ce dataset a moins de colonnes car l'usager est souvent le conducteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-velo-prepare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREPARATION VELO_EDP ===\n",
    "cat = \"velo_edp\"\n",
    "print(f\"=== PREPARATION {cat.upper()} ===\")\n",
    "\n",
    "df_velo = datasets[cat]\n",
    "X_velo, y_velo = prepare_data(df_velo)\n",
    "\n",
    "X_train_velo, X_test_velo, y_train_velo, y_test_velo = train_test_split(\n",
    "    X_velo, y_velo, test_size=0.2, random_state=RANDOM_STATE, stratify=y_velo\n",
    ")\n",
    "\n",
    "print(f\"Features: {X_velo.columns.tolist()}\")\n",
    "print(f\"\\nTrain: {len(X_train_velo):,} | Test: {len(X_test_velo):,}\")\n",
    "print(f\"Taux grave train: {y_train_velo.mean() * 100:.1f}%\")\n",
    "print(f\"Taux grave test: {y_test_velo.mean() * 100:.1f}%\")\n",
    "\n",
    "# Sous-echantillonnage pour Hyperopt\n",
    "sample_size_velo = min(20000, len(X_train_velo))\n",
    "X_sample_velo = X_train_velo.sample(n=sample_size_velo, random_state=RANDOM_STATE)\n",
    "y_sample_velo = y_train_velo.loc[X_sample_velo.index]\n",
    "print(f\"\\nHyperopt sur {sample_size_velo:,} echantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-velo-hyperopt-rf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPEROPT RANDOMFOREST - VELO_EDP ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT RandomForest - VELO_EDP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_rf_velo, best_f1_rf_velo, trials_rf_velo = optimize_boosting_model(\n",
    "    X_sample_velo,\n",
    "    y_sample_velo,\n",
    "    model_type=\"randomforest\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "model_rf_velo = create_pipeline(\n",
    "    RandomForestClassifier(**best_params_rf_velo, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\")\n",
    ")\n",
    "model_rf_velo.fit(X_train_velo, y_train_velo)\n",
    "y_pred_rf_velo = model_rf_velo.predict(X_test_velo)\n",
    "f1_rf_velo_test = f1_score(y_test_velo, y_pred_rf_velo, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_rf_velo_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-velo-hyperopt-xgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPEROPT XGBOOST - VELO_EDP ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT XGBoost - VELO_EDP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_xgb_velo, best_f1_xgb_velo, trials_xgb_velo = optimize_boosting_model(\n",
    "    X_sample_velo,\n",
    "    y_sample_velo,\n",
    "    model_type=\"xgboost\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_velo_imp = pd.DataFrame(imputer.fit_transform(X_train_velo), columns=X_train_velo.columns)\n",
    "X_test_velo_imp = pd.DataFrame(imputer.transform(X_test_velo), columns=X_test_velo.columns)\n",
    "\n",
    "n_neg = (y_train_velo == 0).sum()\n",
    "n_pos = (y_train_velo == 1).sum()\n",
    "scale_pos_weight_velo = n_neg / n_pos\n",
    "\n",
    "model_xgb_velo = XGBClassifier(\n",
    "    **best_params_xgb_velo, random_state=RANDOM_STATE, scale_pos_weight=scale_pos_weight_velo, verbosity=0\n",
    ")\n",
    "model_xgb_velo.fit(X_train_velo_imp, y_train_velo)\n",
    "y_pred_xgb_velo = model_xgb_velo.predict(X_test_velo_imp)\n",
    "f1_xgb_velo_test = f1_score(y_test_velo, y_pred_xgb_velo, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_xgb_velo_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-velo-hyperopt-lgbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPEROPT LIGHTGBM - VELO_EDP ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT LightGBM - VELO_EDP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_lgbm_velo, best_f1_lgbm_velo, trials_lgbm_velo = optimize_boosting_model(\n",
    "    X_sample_velo,\n",
    "    y_sample_velo,\n",
    "    model_type=\"lightgbm\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "model_lgbm_velo = create_pipeline(\n",
    "    LGBMClassifier(**best_params_lgbm_velo, random_state=RANDOM_STATE, class_weight=\"balanced\", verbose=-1)\n",
    ")\n",
    "model_lgbm_velo.fit(X_train_velo, y_train_velo)\n",
    "y_pred_lgbm_velo = model_lgbm_velo.predict(X_test_velo)\n",
    "f1_lgbm_velo_test = f1_score(y_test_velo, y_pred_lgbm_velo, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_lgbm_velo_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-velo-hyperopt-cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPEROPT CATBOOST - VELO_EDP ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT CatBoost - VELO_EDP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_cb_velo, best_f1_cb_velo, trials_cb_velo = optimize_boosting_model(\n",
    "    X_sample_velo,\n",
    "    y_sample_velo,\n",
    "    model_type=\"catboost\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_velo_imp = pd.DataFrame(imputer.fit_transform(X_train_velo), columns=X_train_velo.columns)\n",
    "X_test_velo_imp = pd.DataFrame(imputer.transform(X_test_velo), columns=X_test_velo.columns)\n",
    "\n",
    "model_cb_velo = CatBoostClassifier(\n",
    "    **best_params_cb_velo,\n",
    "    random_state=RANDOM_STATE,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "model_cb_velo.fit(X_train_velo_imp, y_train_velo)\n",
    "y_pred_cb_velo = model_cb_velo.predict(X_test_velo_imp)\n",
    "f1_cb_velo_test = f1_score(y_test_velo, y_pred_cb_velo, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_cb_velo_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-velo-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPARAISON VELO_EDP ===\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARAISON DES MODELES - VELO_EDP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models_eval_velo = []\n",
    "\n",
    "# RF optimise\n",
    "model = create_pipeline(\n",
    "    RandomForestClassifier(**best_params_rf_velo, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\")\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_model(model, X_train_velo, X_test_velo, y_train_velo, y_test_velo, \"RF_HyperOpt\")\n",
    "models_eval_velo.append({\"name\": \"RF_HyperOpt\", \"y_true\": y_test_velo, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# XGBoost optimise\n",
    "model = create_pipeline(\n",
    "    XGBClassifier(\n",
    "        **best_params_xgb_velo, random_state=RANDOM_STATE, n_jobs=nb_workers, scale_pos_weight=scale_pos_weight_velo\n",
    "    )\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_model(model, X_train_velo, X_test_velo, y_train_velo, y_test_velo, \"XGBoost\")\n",
    "models_eval_velo.append({\"name\": \"XGBoost\", \"y_true\": y_test_velo, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# LightGBM optimise\n",
    "model = create_pipeline(\n",
    "    LGBMClassifier(\n",
    "        **best_params_lgbm_velo, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\", verbose=-1\n",
    "    )\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_model(model, X_train_velo, X_test_velo, y_train_velo, y_test_velo, \"LightGBM\")\n",
    "models_eval_velo.append({\"name\": \"LightGBM\", \"y_true\": y_test_velo, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# CatBoost optimise\n",
    "model_cb = CatBoostClassifier(\n",
    "    **best_params_cb_velo,\n",
    "    random_state=RANDOM_STATE,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_catboost(model_cb, X_train_velo, X_test_velo, y_train_velo, y_test_velo, \"CatBoost\")\n",
    "models_eval_velo.append({\"name\": \"CatBoost\", \"y_true\": y_test_velo, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# Affichage\n",
    "display_metrics(models_results=models_eval_velo, class_labels=[\"Non grave\", \"Grave\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-velo-select",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SELECTION MEILLEUR MODELE VELO_EDP ===\n",
    "best_model_velo = select_best_model(models_eval_velo, metric=\"f1\")\n",
    "print(f\"\\nMeilleur modele VELO_EDP: {best_model_velo['name']}\")\n",
    "print(f\"F1-Score: {best_model_velo['score']:.4f}\")\n",
    "print(f\"Amelioration vs baseline (0.557): {(best_model_velo['score'] - 0.557) * 100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-pieton-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Fine-tuning PIETON\n",
    "\n",
    "### Caracteristiques de cette categorie\n",
    "\n",
    "| Caracteristique | Valeur |\n",
    "|-----------------|--------|\n",
    "| Taille dataset | ~46k lignes |\n",
    "| Taux de gravite | ~33% |\n",
    "| F1 baseline (4a) | 0.596 |\n",
    "| Objectif | Depasser 0.596 |\n",
    "\n",
    "**Note**: Les pietons n'ont pas de vehicule, donc moins de features liees au vehicule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pieton-prepare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREPARATION PIETON ===\n",
    "cat = \"pieton\"\n",
    "print(f\"=== PREPARATION {cat.upper()} ===\")\n",
    "\n",
    "df_pieton = datasets[cat]\n",
    "X_pieton, y_pieton = prepare_data(df_pieton)\n",
    "\n",
    "X_train_pieton, X_test_pieton, y_train_pieton, y_test_pieton = train_test_split(\n",
    "    X_pieton, y_pieton, test_size=0.2, random_state=RANDOM_STATE, stratify=y_pieton\n",
    ")\n",
    "\n",
    "print(f\"Features: {X_pieton.columns.tolist()}\")\n",
    "print(f\"\\nTrain: {len(X_train_pieton):,} | Test: {len(X_test_pieton):,}\")\n",
    "print(f\"Taux grave train: {y_train_pieton.mean() * 100:.1f}%\")\n",
    "print(f\"Taux grave test: {y_test_pieton.mean() * 100:.1f}%\")\n",
    "\n",
    "# Sous-echantillonnage pour Hyperopt\n",
    "sample_size_pieton = min(25000, len(X_train_pieton))\n",
    "X_sample_pieton = X_train_pieton.sample(n=sample_size_pieton, random_state=RANDOM_STATE)\n",
    "y_sample_pieton = y_train_pieton.loc[X_sample_pieton.index]\n",
    "print(f\"\\nHyperopt sur {sample_size_pieton:,} echantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pieton-hyperopt-rf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPEROPT RANDOMFOREST - PIETON ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT RandomForest - PIETON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_rf_pieton, best_f1_rf_pieton, trials_rf_pieton = optimize_boosting_model(\n",
    "    X_sample_pieton,\n",
    "    y_sample_pieton,\n",
    "    model_type=\"randomforest\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "model_rf_pieton = create_pipeline(\n",
    "    RandomForestClassifier(\n",
    "        **best_params_rf_pieton, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\"\n",
    "    )\n",
    ")\n",
    "model_rf_pieton.fit(X_train_pieton, y_train_pieton)\n",
    "y_pred_rf_pieton = model_rf_pieton.predict(X_test_pieton)\n",
    "f1_rf_pieton_test = f1_score(y_test_pieton, y_pred_rf_pieton, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_rf_pieton_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pieton-hyperopt-xgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPEROPT XGBOOST - PIETON ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT XGBoost - PIETON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_xgb_pieton, best_f1_xgb_pieton, trials_xgb_pieton = optimize_boosting_model(\n",
    "    X_sample_pieton,\n",
    "    y_sample_pieton,\n",
    "    model_type=\"xgboost\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_pieton_imp = pd.DataFrame(imputer.fit_transform(X_train_pieton), columns=X_train_pieton.columns)\n",
    "X_test_pieton_imp = pd.DataFrame(imputer.transform(X_test_pieton), columns=X_test_pieton.columns)\n",
    "\n",
    "n_neg = (y_train_pieton == 0).sum()\n",
    "n_pos = (y_train_pieton == 1).sum()\n",
    "scale_pos_weight_pieton = n_neg / n_pos\n",
    "\n",
    "model_xgb_pieton = XGBClassifier(\n",
    "    **best_params_xgb_pieton, random_state=RANDOM_STATE, scale_pos_weight=scale_pos_weight_pieton, verbosity=0\n",
    ")\n",
    "model_xgb_pieton.fit(X_train_pieton_imp, y_train_pieton)\n",
    "y_pred_xgb_pieton = model_xgb_pieton.predict(X_test_pieton_imp)\n",
    "f1_xgb_pieton_test = f1_score(y_test_pieton, y_pred_xgb_pieton, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_xgb_pieton_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pieton-hyperopt-lgbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPEROPT LIGHTGBM - PIETON ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT LightGBM - PIETON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_lgbm_pieton, best_f1_lgbm_pieton, trials_lgbm_pieton = optimize_boosting_model(\n",
    "    X_sample_pieton,\n",
    "    y_sample_pieton,\n",
    "    model_type=\"lightgbm\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "model_lgbm_pieton = create_pipeline(\n",
    "    LGBMClassifier(**best_params_lgbm_pieton, random_state=RANDOM_STATE, class_weight=\"balanced\", verbose=-1)\n",
    ")\n",
    "model_lgbm_pieton.fit(X_train_pieton, y_train_pieton)\n",
    "y_pred_lgbm_pieton = model_lgbm_pieton.predict(X_test_pieton)\n",
    "f1_lgbm_pieton_test = f1_score(y_test_pieton, y_pred_lgbm_pieton, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_lgbm_pieton_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pieton-hyperopt-cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPEROPT CATBOOST - PIETON ===\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPEROPT CatBoost - PIETON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_params_cb_pieton, best_f1_cb_pieton, trials_cb_pieton = optimize_boosting_model(\n",
    "    X_sample_pieton,\n",
    "    y_sample_pieton,\n",
    "    model_type=\"catboost\",\n",
    "    max_evals=max_evals,\n",
    "    cv=3,\n",
    "    scoring=\"f1\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=nb_workers,\n",
    ")\n",
    "\n",
    "print(\"\\nValidation sur test set complet...\")\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_pieton_imp = pd.DataFrame(imputer.fit_transform(X_train_pieton), columns=X_train_pieton.columns)\n",
    "X_test_pieton_imp = pd.DataFrame(imputer.transform(X_test_pieton), columns=X_test_pieton.columns)\n",
    "\n",
    "model_cb_pieton = CatBoostClassifier(\n",
    "    **best_params_cb_pieton,\n",
    "    random_state=RANDOM_STATE,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "model_cb_pieton.fit(X_train_pieton_imp, y_train_pieton)\n",
    "y_pred_cb_pieton = model_cb_pieton.predict(X_test_pieton_imp)\n",
    "f1_cb_pieton_test = f1_score(y_test_pieton, y_pred_cb_pieton, average=\"weighted\")\n",
    "print(f\"F1 sur test set: {f1_cb_pieton_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pieton-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPARAISON PIETON ===\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARAISON DES MODELES - PIETON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models_eval_pieton = []\n",
    "\n",
    "# RF optimise\n",
    "model = create_pipeline(\n",
    "    RandomForestClassifier(\n",
    "        **best_params_rf_pieton, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\"\n",
    "    )\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_model(\n",
    "    model, X_train_pieton, X_test_pieton, y_train_pieton, y_test_pieton, \"RF_HyperOpt\"\n",
    ")\n",
    "models_eval_pieton.append({\"name\": \"RF_HyperOpt\", \"y_true\": y_test_pieton, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# XGBoost optimise\n",
    "model = create_pipeline(\n",
    "    XGBClassifier(\n",
    "        **best_params_xgb_pieton, random_state=RANDOM_STATE, n_jobs=nb_workers, scale_pos_weight=scale_pos_weight_pieton\n",
    "    )\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_model(model, X_train_pieton, X_test_pieton, y_train_pieton, y_test_pieton, \"XGBoost\")\n",
    "models_eval_pieton.append({\"name\": \"XGBoost\", \"y_true\": y_test_pieton, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# LightGBM optimise\n",
    "model = create_pipeline(\n",
    "    LGBMClassifier(\n",
    "        **best_params_lgbm_pieton, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\", verbose=-1\n",
    "    )\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_model(model, X_train_pieton, X_test_pieton, y_train_pieton, y_test_pieton, \"LightGBM\")\n",
    "models_eval_pieton.append({\"name\": \"LightGBM\", \"y_true\": y_test_pieton, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# CatBoost optimise\n",
    "model_cb = CatBoostClassifier(\n",
    "    **best_params_cb_pieton,\n",
    "    random_state=RANDOM_STATE,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "res, y_pred, y_proba = evaluate_catboost(\n",
    "    model_cb, X_train_pieton, X_test_pieton, y_train_pieton, y_test_pieton, \"CatBoost\"\n",
    ")\n",
    "models_eval_pieton.append({\"name\": \"CatBoost\", \"y_true\": y_test_pieton, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
    "\n",
    "# Affichage\n",
    "display_metrics(models_results=models_eval_pieton, class_labels=[\"Non grave\", \"Grave\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pieton-select",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SELECTION MEILLEUR MODELE PIETON ===\n",
    "best_model_pieton = select_best_model(models_eval_pieton, metric=\"f1\")\n",
    "print(f\"\\nMeilleur modele PIETON: {best_model_pieton['name']}\")\n",
    "print(f\"F1-Score: {best_model_pieton['score']:.4f}\")\n",
    "print(f\"Amelioration vs baseline (0.596): {(best_model_pieton['score'] - 0.596) * 100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Resume et sauvegarde des modeles optimises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RESUME GLOBAL ===\n",
    "print(\"=\" * 70)\n",
    "print(\"RESUME DES RESULTATS FINE-TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Categorie':<20} {'Baseline F1':>12} {'Best Model':>15} {'Best F1':>10} {'Gain':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "results_summary = [\n",
    "    (\"vehicule_leger\", 0.640, best_model_vl[\"name\"], best_model_vl[\"score\"]),\n",
    "    (\"velo_edp\", 0.557, best_model_velo[\"name\"], best_model_velo[\"score\"]),\n",
    "    (\"pieton\", 0.596, best_model_pieton[\"name\"], best_model_pieton[\"score\"]),\n",
    "]\n",
    "\n",
    "for cat, baseline, model_name, score in results_summary:\n",
    "    gain = (score - baseline) * 100\n",
    "    print(f\"{cat:<20} {baseline:>12.3f} {model_name:>15} {score:>10.3f} {gain:>+9.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save-vl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAUVEGARDE MODELE VEHICULE_LEGER ===\n",
    "print(\"=\" * 70)\n",
    "print(\"SAUVEGARDE MODELE VEHICULE_LEGER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_vl_full, y_vl_full = prepare_data(datasets[\"vehicule_leger\"])\n",
    "pos_weight_vl = (y_vl_full == 0).sum() / (y_vl_full == 1).sum()\n",
    "\n",
    "model_configs_vl = {\n",
    "    \"RF_HyperOpt\": lambda: create_pipeline(\n",
    "        RandomForestClassifier(\n",
    "            **best_params_rf_vl, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\"\n",
    "        )\n",
    "    ),\n",
    "    \"XGBoost\": lambda: create_pipeline(\n",
    "        XGBClassifier(\n",
    "            **best_params_xgb_vl, random_state=RANDOM_STATE, n_jobs=nb_workers, scale_pos_weight=pos_weight_vl\n",
    "        )\n",
    "    ),\n",
    "    \"LightGBM\": lambda: create_pipeline(\n",
    "        LGBMClassifier(\n",
    "            **best_params_lgbm_vl, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\", verbose=-1\n",
    "        )\n",
    "    ),\n",
    "    \"CatBoost\": lambda: CatBoostClassifier(\n",
    "        **best_params_cb_vl,\n",
    "        random_state=RANDOM_STATE,\n",
    "        auto_class_weights=\"Balanced\",\n",
    "        verbose=False,\n",
    "        allow_writing_files=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "result_vl = save_best_model(\n",
    "    best_model_name=best_model_vl[\"name\"],\n",
    "    model_configs=model_configs_vl,\n",
    "    X_full=X_vl_full,\n",
    "    y_full=y_vl_full,\n",
    "    X_test=X_test_vl,\n",
    "    y_test=y_test_vl,\n",
    "    save_path=\"models/model_passager_vehicule_leger_optimized.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save-velo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAUVEGARDE MODELE VELO_EDP ===\n",
    "print(\"=\" * 70)\n",
    "print(\"SAUVEGARDE MODELE VELO_EDP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_velo_full, y_velo_full = prepare_data(datasets[\"velo_edp\"])\n",
    "pos_weight_velo = (y_velo_full == 0).sum() / (y_velo_full == 1).sum()\n",
    "\n",
    "model_configs_velo = {\n",
    "    \"RF_HyperOpt\": lambda: create_pipeline(\n",
    "        RandomForestClassifier(\n",
    "            **best_params_rf_velo, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\"\n",
    "        )\n",
    "    ),\n",
    "    \"XGBoost\": lambda: create_pipeline(\n",
    "        XGBClassifier(\n",
    "            **best_params_xgb_velo, random_state=RANDOM_STATE, n_jobs=nb_workers, scale_pos_weight=pos_weight_velo\n",
    "        )\n",
    "    ),\n",
    "    \"LightGBM\": lambda: create_pipeline(\n",
    "        LGBMClassifier(\n",
    "            **best_params_lgbm_velo, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\", verbose=-1\n",
    "        )\n",
    "    ),\n",
    "    \"CatBoost\": lambda: CatBoostClassifier(\n",
    "        **best_params_cb_velo,\n",
    "        random_state=RANDOM_STATE,\n",
    "        auto_class_weights=\"Balanced\",\n",
    "        verbose=False,\n",
    "        allow_writing_files=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "result_velo = save_best_model(\n",
    "    best_model_name=best_model_velo[\"name\"],\n",
    "    model_configs=model_configs_velo,\n",
    "    X_full=X_velo_full,\n",
    "    y_full=y_velo_full,\n",
    "    X_test=X_test_velo,\n",
    "    y_test=y_test_velo,\n",
    "    save_path=\"models/model_passager_velo_edp_optimized.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save-pieton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAUVEGARDE MODELE PIETON ===\n",
    "print(\"=\" * 70)\n",
    "print(\"SAUVEGARDE MODELE PIETON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_pieton_full, y_pieton_full = prepare_data(datasets[\"pieton\"])\n",
    "pos_weight_pieton = (y_pieton_full == 0).sum() / (y_pieton_full == 1).sum()\n",
    "\n",
    "model_configs_pieton = {\n",
    "    \"RF_HyperOpt\": lambda: create_pipeline(\n",
    "        RandomForestClassifier(\n",
    "            **best_params_rf_pieton, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\"\n",
    "        )\n",
    "    ),\n",
    "    \"XGBoost\": lambda: create_pipeline(\n",
    "        XGBClassifier(\n",
    "            **best_params_xgb_pieton, random_state=RANDOM_STATE, n_jobs=nb_workers, scale_pos_weight=pos_weight_pieton\n",
    "        )\n",
    "    ),\n",
    "    \"LightGBM\": lambda: create_pipeline(\n",
    "        LGBMClassifier(\n",
    "            **best_params_lgbm_pieton, random_state=RANDOM_STATE, n_jobs=nb_workers, class_weight=\"balanced\", verbose=-1\n",
    "        )\n",
    "    ),\n",
    "    \"CatBoost\": lambda: CatBoostClassifier(\n",
    "        **best_params_cb_pieton,\n",
    "        random_state=RANDOM_STATE,\n",
    "        auto_class_weights=\"Balanced\",\n",
    "        verbose=False,\n",
    "        allow_writing_files=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "result_pieton = save_best_model(\n",
    "    best_model_name=best_model_pieton[\"name\"],\n",
    "    model_configs=model_configs_pieton,\n",
    "    X_full=X_pieton_full,\n",
    "    y_full=y_pieton_full,\n",
    "    X_test=X_test_pieton,\n",
    "    y_test=y_test_pieton,\n",
    "    save_path=\"models/model_passager_pieton_optimized.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save-hyperparams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAUVEGARDE DES HYPERPARAMETRES OPTIMAUX ===\n",
    "hyperparams_all = {\n",
    "    \"vehicule_leger\": {\n",
    "        \"best_model\": best_model_vl[\"name\"],\n",
    "        \"best_f1\": best_model_vl[\"score\"],\n",
    "        \"params_rf\": best_params_rf_vl,\n",
    "        \"params_xgb\": best_params_xgb_vl,\n",
    "        \"params_lgbm\": best_params_lgbm_vl,\n",
    "        \"params_cb\": best_params_cb_vl,\n",
    "    },\n",
    "    \"velo_edp\": {\n",
    "        \"best_model\": best_model_velo[\"name\"],\n",
    "        \"best_f1\": best_model_velo[\"score\"],\n",
    "        \"params_rf\": best_params_rf_velo,\n",
    "        \"params_xgb\": best_params_xgb_velo,\n",
    "        \"params_lgbm\": best_params_lgbm_velo,\n",
    "        \"params_cb\": best_params_cb_velo,\n",
    "    },\n",
    "    \"pieton\": {\n",
    "        \"best_model\": best_model_pieton[\"name\"],\n",
    "        \"best_f1\": best_model_pieton[\"score\"],\n",
    "        \"params_rf\": best_params_rf_pieton,\n",
    "        \"params_xgb\": best_params_xgb_pieton,\n",
    "        \"params_lgbm\": best_params_lgbm_pieton,\n",
    "        \"params_cb\": best_params_cb_pieton,\n",
    "    },\n",
    "}\n",
    "\n",
    "joblib.dump(hyperparams_all, \"models/hyperparams_passagers_finetuned.joblib\")\n",
    "print(\"Hyperparametres sauvegardes: models/hyperparams_passagers_finetuned.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume et conclusions\n",
    "\n",
    "### Modeles sauvegardes\n",
    "\n",
    "| Fichier | Categorie | Usage |\n",
    "|---------|-----------|-------|\n",
    "| `model_passager_vehicule_leger_optimized.joblib` | 2RM, quads | **Production** |\n",
    "| `model_passager_velo_edp_optimized.joblib` | Velos, trottinettes | **Production** |\n",
    "| `model_passager_pieton_optimized.joblib` | Pietons | **Production** |\n",
    "| `model_passager_global.joblib` (notebook 4a) | Voitures, poids lourds | **Production** |\n",
    "\n",
    "### Architecture de prediction recommandee\n",
    "\n",
    "```python\n",
    "def predict_gravite_passager(features, categorie_vehicule):\n",
    "    if categorie_vehicule in ['vehicule_leger', 'velo_edp', 'pieton']:\n",
    "        # Utiliser le modele specialise\n",
    "        model = load(f'model_passager_{categorie_vehicule}_optimized.joblib')\n",
    "    else:\n",
    "        # Voiture ou poids_lourd -> modele global\n",
    "        model = load('model_passager_global.joblib')\n",
    "    return model.predict(features)\n",
    "```\n",
    "\n",
    "### Prochaines etapes\n",
    "\n",
    "1. **Deploiement API** : Implementer la logique de routing ci-dessus\n",
    "2. **Monitoring** : Suivre les performances par categorie en production\n",
    "3. **Reentrainement** : Planifier un reentrainement periodique avec nouvelles donnees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
